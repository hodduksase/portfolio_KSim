# ============================================
# 2013~2015 ê¸°ì‚¬ LDA ë¶„ì„ (ì—°ë„ë³„ 5ê°œì”© í† í”½)
# ============================================

import os, re, warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import time

def simple_tokenize(text):
    """ê°„ë‹¨í•œ í† í°í™” (Java ì—†ì´)"""
    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
    text = re.sub(r'[^ê°€-í£A-Za-z0-9\s]', ' ', str(text))
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text).strip()
    # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì¶”ì¶œ
    tokens = [word for word in text.split() if len(word) >= 2]
    return tokens

def analyze_year_data(file_path, year):
    """ì—°ë„ë³„ ê¸°ì‚¬ ë°ì´í„° LDA ë¶„ì„"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {year}ë…„ ê¸°ì‚¬ LDA ë¶„ì„ ì‹œì‘")
    print(f"{'='*60}")
    
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        return None
    
    print(f"âœ… íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
    df = pd.read_excel(file_path)
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    print(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
    
    # ì œëª©/ë³¸ë¬¸ ì»¬ëŸ¼ ìë™ íƒìƒ‰
    def find_col(df, candidates):
        for c in candidates:
            if c in df.columns: 
                return c
        return None

    TITLE_COL   = find_col(df, ["ì œëª©","title","Title","ê¸°ì‚¬ì œëª©"])
    CONTENT_COL = find_col(df, ["ë³¸ë¬¸","content","Content","ê¸°ì‚¬ë³¸ë¬¸","ê¸°ì‚¬ë‚´ìš©","ìš”ì•½"])
    
    if not TITLE_COL and not CONTENT_COL:
        print(f"âŒ ì œëª©/ë³¸ë¬¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"âœ… ì»¬ëŸ¼ ì°¾ê¸° ì™„ë£Œ:")
    print(f"  ì œëª© ì»¬ëŸ¼: {TITLE_COL}")
    print(f"  ë³¸ë¬¸ ì»¬ëŸ¼: {CONTENT_COL}")
    
    # í…ìŠ¤íŠ¸ ê²°í•©
    texts = (df[TITLE_COL].fillna("") if TITLE_COL else pd.Series([""]*len(df))) + " " + \
            (df[CONTENT_COL].fillna("") if CONTENT_COL else pd.Series([""]*len(df)))
    texts = texts.astype(str)
    
    print(f"ğŸ“ í…ìŠ¤íŠ¸ ê²°í•© ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
    print("ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™” ì¤‘...")
    
    tokenized_docs = []
    for i, text in enumerate(texts):
        if i % 1000 == 0:
            progress = (i / len(texts)) * 100
            print(f"  ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(texts)})")
        
        tokens = simple_tokenize(text)
        if tokens:  # ë¹ˆ í† í° ë¦¬ìŠ¤íŠ¸ ì œì™¸
            tokenized_docs.append(tokens)
    
    print(f"âœ… í† í°í™” ì™„ë£Œ: {len(tokenized_docs)}ê°œ ìœ íš¨ ë¬¸ì„œ")
    
    if len(tokenized_docs) < 10:
        print("âŒ í† í°í™”ëœ ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
        return None
    
    # DTM êµ¬ì„±
    print("ğŸ“Š DTM êµ¬ì„± ì¤‘...")
    
    # í† í°ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
    text_for_vectorizer = [' '.join(doc) for doc in tokenized_docs]
    
    vectorizer = CountVectorizer(
        min_df=5,  # ìµœì†Œ 5ë²ˆ ì´ìƒ ë“±ì¥
        max_df=0.5,  # ìµœëŒ€ 50% ì´í•˜ ë¬¸ì„œì—ì„œ ë“±ì¥
        max_features=200  # ìµœëŒ€ 200ê°œ ë‹¨ì–´
    )
    
    # DTM ìƒì„±
    dtm = vectorizer.fit_transform(text_for_vectorizer)
    print(f"âœ… DTM ìƒì„± ì™„ë£Œ: {dtm.shape}")
    
    # LDA í•™ìŠµ
    print("ğŸ¤– LDA ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    n_topics = 5
    lda = LatentDirichletAllocation(
        n_components=n_topics, 
        learning_method="batch",
        max_iter=20, 
        random_state=42, 
        evaluate_every=0, 
        n_jobs=1  # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©
    )
    
    # ëª¨ë¸ í•™ìŠµ ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    lda.fit(dtm)
    training_time = time.time() - start_time
    
    print(f"âœ… LDA ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")
    
    # í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶œë ¥
    print(f"\nğŸ“Š {year}ë…„ LDA í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ")
    print("-" * 50)
    
    feature_names = vectorizer.get_feature_names_out()
    topn = min(10, len(feature_names))
    
    topics = []
    for idx, comp in enumerate(lda.components_):
        top_ids = comp.argsort()[-topn:][::-1]
        words = [feature_names[i] for i in top_ids]
        weights = [comp[i] for i in top_ids]
        
        print(f"\nğŸ” í† í”½ {idx+1}:")
        topic_str = f"Topic #{idx+1}: "
        for i, (word, weight) in enumerate(zip(words, weights)):
            print(f"  {i+1:2d}. {word:15s} (ê°€ì¤‘ì¹˜: {weight:.6f})")
            if i < 10:  # ìƒìœ„ 10ê°œë§Œ
                topic_str += f"{weight:.3f}*\"{word}\" + "
        
        topic_str = topic_str.rstrip(" + ")
        topics.append(topic_str)
    
    # ì„±ëŠ¥ ì •ë³´
    print(f"\nğŸ“ˆ {year}ë…„ ì„±ëŠ¥ ì •ë³´")
    print("-" * 50)
    print(f"  ì´ ë¬¸ì„œ ìˆ˜: {len(tokenized_docs)}ê°œ")
    print(f"  ì–´íœ˜ í¬ê¸°: {dtm.shape[1]}ê°œ")
    print(f"  í† í”½ ìˆ˜: {n_topics}ê°œ")
    print(f"  í•™ìŠµ ì‹œê°„: {training_time:.2f}ì´ˆ")
    print(f"  í‰ê·  ë¬¸ì„œë‹¹ í† í° ìˆ˜: {np.mean([len(doc) for doc in tokenized_docs]):.1f}ê°œ")
    
    return topics

def main():
    print("ğŸš€ 2013~2015ë…„ ê¸°ì‚¬ LDA ë¶„ì„ ì‹œì‘")
    
    # ì—°ë„ë³„ íŒŒì¼ ê²½ë¡œ
    files = {
        2013: "NewsResult_20130101-20131231.xlsx",
        2014: "NewsResult_20140101-20141231.xlsx", 
        2015: "NewsResult_20150101-20151231.xlsx"
    }
    
    all_topics = {}
    
    # ê° ì—°ë„ë³„ ë¶„ì„ ìˆ˜í–‰
    for year, file_path in files.items():
        topics = analyze_year_data(file_path, year)
        if topics:
            all_topics[year] = topics
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“Š 2013~2015ë…„ LDA ë¶„ì„ ì „ì²´ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}")
    
    for year, topics in all_topics.items():
        print(f"\nğŸ† {year}ë…„ LDA Topics:")
        for topic in topics:
            print(topic)
    
    print(f"\nâœ… ëª¨ë“  ì—°ë„ LDA ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_topics)}ê°œ ì—°ë„, {sum(len(topics) for topics in all_topics.values())}ê°œ í† í”½ ë¶„ì„")

if __name__ == "__main__":
    main()


